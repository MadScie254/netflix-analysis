{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0581e2",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "### Data Preparation:\n",
    "\n",
    "1. **Handling Missing Values:**\n",
    "   - Check the dataset for any missing values and decide on an appropriate strategy for handling them. This could involve imputation (e.g., filling missing values with the mean, median, or mode) or removal of rows or columns with missing values, depending on the data and the extent of missingness.\n",
    "\n",
    "2. **Scaling Numerical Features:**\n",
    "   - Scale numerical features to ensure that they have similar scales, which can improve the performance of certain machine learning algorithms, especially those sensitive to feature magnitudes (e.g., gradient descent-based algorithms).\n",
    "   - Common scaling techniques include standardization (scaling to have zero mean and unit variance) or normalization (scaling to a range, typically [0, 1]).\n",
    "\n",
    "3. **Splitting the Dataset:**\n",
    "   - Split the dataset into training and testing sets to train and evaluate the machine learning models.\n",
    "   - Typically, a common split is 80% for training and 20% for testing, but the split ratio can be adjusted based on the size of the dataset and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23dfa30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Date         0\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Adj Close    0\n",
      "Volume       0\n",
      "dtype: int64\n",
      "Shape of X_train: (4028, 5)\n",
      "Shape of X_test: (1007, 5)\n",
      "Shape of y_train: (4028,)\n",
      "Shape of y_test: (1007,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the historical data\n",
    "netflix_data = pd.read_csv(\"cleaned_netflix_data.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = netflix_data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = netflix_data.drop(columns=['Date', 'Adj Close'])  \n",
    "y = netflix_data['Adj Close']  \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a37edf",
   "metadata": {},
   "source": [
    "## Feature Selection:\n",
    "Identify Relevant Features:\n",
    "\n",
    "**Historical Financial Data:** Features such as 'Open', 'High', 'Low', 'Close', and 'Volume' are essential as they directly influence share prices and can provide insights into market trends and trading activity.\n",
    "\n",
    "**Calendar Features:** Incorporate calendar-related features such as day of the week, month, or quarter, which may influence trading behavior and market dynamics.\n",
    "\n",
    "**Create Lag Features:** Generate lagged versions of the existing features to capture temporal dependencies and trends in the data. For example, lagged values of 'Close' prices from previous days could be informative for predicting future prices.\n",
    "\n",
    "**Rolling Window Statistics:** Compute rolling window statistics (moving averages, standard deviations) over different time periods to capture short-term and long-term trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992df72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Open      High       Low     Close  Adj Close    Volume  \\\n",
      "0  2004-01-26  5.464286  5.505714  5.375000  5.447143   5.447143  31306800   \n",
      "1  2004-01-27  5.428571  5.681429  5.350714  5.370714   5.370714  67012400   \n",
      "2  2004-01-28  5.365714  5.455714  5.053571  5.135714   5.135714  49611800   \n",
      "3  2004-01-29  5.209286  5.216429  4.906429  5.053571   5.053571  56393400   \n",
      "4  2004-01-30  5.033571  5.333571  5.013571  5.243571   5.243571  35644000   \n",
      "\n",
      "   Close_Lag1  Close_Lag7  Rolling_Mean_Close  Rolling_Std_Close  Day_of_Week  \\\n",
      "0         NaN         NaN                 NaN                NaN            0   \n",
      "1    5.447143         NaN                 NaN                NaN            1   \n",
      "2    5.370714         NaN                 NaN                NaN            2   \n",
      "3    5.135714         NaN                 NaN                NaN            3   \n",
      "4    5.053571         NaN                 NaN                NaN            4   \n",
      "\n",
      "   Month  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the historical data\n",
    "netflix_data = pd.read_csv(\"cleaned_netflix_data.csv\")\n",
    "\n",
    "# Feature Engineering\n",
    "# Create Lag Features\n",
    "netflix_data['Close_Lag1'] = netflix_data['Close'].shift(1)  # Lagged Close price from previous day\n",
    "netflix_data['Close_Lag7'] = netflix_data['Close'].shift(7)  # Lagged Close price from a week ago\n",
    "\n",
    "# Rolling Window Statistics\n",
    "netflix_data['Rolling_Mean_Close'] = netflix_data['Close'].rolling(window=30).mean()  # 30-day moving average of Close prices\n",
    "netflix_data['Rolling_Std_Close'] = netflix_data['Close'].rolling(window=30).std()    # 30-day rolling standard deviation of Close prices\n",
    "\n",
    "# Calendar Features\n",
    "netflix_data['Day_of_Week'] = pd.to_datetime(netflix_data['Date']).dt.dayofweek  # Day of the week (0 = Monday, 6 = Sunday)\n",
    "netflix_data['Month'] = pd.to_datetime(netflix_data['Date']).dt.month              # Month of the year (1 to 12)\n",
    "\n",
    "# Display the updated dataframe with new features\n",
    "print(netflix_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a25a50",
   "metadata": {},
   "source": [
    "## Model -- random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb35877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "   Feature  Importance\n",
      "3   Close    0.936211\n",
      "1    High    0.023610\n",
      "2     Low    0.023396\n",
      "0    Open    0.016782\n",
      "4  Volume    0.000001\n",
      "\n",
      "Validation Set Mean Squared Error (Random Forest): 0.08711301187256565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Adjust these based on your actual data and features\n",
    "X = netflix_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "y = netflix_data['Adj Close']\n",
    "\n",
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "n_estimators = 100\n",
    "max_depth = 10\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Model Training\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature Importance:\\n\", feature_importance)\n",
    "\n",
    "# Model Evaluation on Validation Set\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "\n",
    "print(\"\\nValidation Set Mean Squared Error (Random Forest):\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5137e",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d57faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "\n",
      "Validation Set Mean Squared Error (Random Forest): 0.0754588208173375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data (replace 'filename.csv' with your actual file)\n",
    "netflix_data = pd.read_csv('netflix.csv')\n",
    "\n",
    "# Adjust these based on your actual data and features\n",
    "X = netflix_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "y = netflix_data['Adj Close']\n",
    "\n",
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search Cross-Validation for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by Grid Search\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Model Evaluation on Validation Set\n",
    "y_val_pred_rf = best_rf_model.predict(X_val)\n",
    "mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "\n",
    "print(\"\\nValidation Set Mean Squared Error (Random Forest):\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec9549",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02f5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "netflix_data = pd.read_csv('netflix.csv')\n",
    "\n",
    "# Adjust these based on your actual data and features\n",
    "X = netflix_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "y = netflix_data['Adj Close']\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=150, max_depth=None, min_samples_leaf=2, min_samples_split=2, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Save the trained model using joblib\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77600ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
